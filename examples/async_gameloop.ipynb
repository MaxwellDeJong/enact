{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dataclasses\n",
    "import enact\n",
    "\n",
    "def normalize_angle(theta: np.ndarray) -> np.ndarray:\n",
    "  return (theta + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "@enact.register\n",
    "@dataclasses.dataclass\n",
    "class Action(enact.Resource):\n",
    "  array: np.ndarray = np.zeros((1, 2))\n",
    "\n",
    "  @property\n",
    "  def torque(self) -> np.ndarray:\n",
    "    return self.array[..., 0]\n",
    "\n",
    "  @property\n",
    "  def thrust(self) -> np.ndarray:\n",
    "    return self.array[..., 1]\n",
    "\n",
    "@enact.register\n",
    "@dataclasses.dataclass\n",
    "class State(enact.Resource):\n",
    "  \"\"\"Represents a game state.\n",
    "\n",
    "  The state is represented internally as a numpy array of length 10.\n",
    "  State components can be accessed with setters and getters:\n",
    "  * Agent position\n",
    "  * Agent orientation [-pi, pi]\n",
    "  * Agent velocity\n",
    "  * Agent angular speed\n",
    "  * Goal position\n",
    "  * Has been at goal\n",
    "\n",
    "  A state object may represent an arbitrary batch of states.\n",
    "  \"\"\"\n",
    "\n",
    "  # Indices and slices into the underlying arrays.\n",
    "  POSITION_SLICE = slice(0, 2)\n",
    "  ROTATION_INDEX = 2\n",
    "  VELOCITY_SLICE = slice(3, 5)\n",
    "  ANGULAR_VELOCITY_INDEX = 5\n",
    "  GOAL1_POSITION_SLICE = slice(6, 8)\n",
    "  HAS_BEEN_AT_GOAL1_INDEX = 8\n",
    "  GOAL2_POSITION_SLICE = slice(9, 11)\n",
    "  HAS_BEEN_AT_GOAL2_INDEX = 11\n",
    "  # Total dimensionality of the state space array.\n",
    "  ARRAY_SIZE = 12\n",
    "\n",
    "  # Game configuration.\n",
    "  BOARD_SIZE = 25\n",
    "  REACHED_EPSILON = 1\n",
    "  TORQUE_FORCE = 0.5\n",
    "  THRUST_FORCE = 0.5\n",
    "  MAX_SPEED = 2\n",
    "  FRICTION_COEFFICIENT = 0.99\n",
    "\n",
    "  array: np.ndarray = np.zeros((1, ARRAY_SIZE))\n",
    "\n",
    "  def randomize(self):\n",
    "    batch_shape = self.array.shape[:-1]\n",
    "    self.position = np.random.uniform(0, State.BOARD_SIZE, batch_shape + (2,))\n",
    "    self.rotation = np.random.uniform(-np.pi, np.pi, batch_shape)\n",
    "    self.goal1_position = np.random.uniform(\n",
    "      0, State.BOARD_SIZE, batch_shape + (2,))\n",
    "    self.goal2_position = np.random.uniform(\n",
    "      0, State.BOARD_SIZE, batch_shape + (2,))\n",
    "    self.success = np.zeros_like(self.rotation)\n",
    "\n",
    "  @property\n",
    "  def position(self) -> np.ndarray:\n",
    "    \"\"\"Return the position of the agent.\"\"\"\n",
    "    return self.array[..., State.POSITION_SLICE]\n",
    "\n",
    "  @position.setter\n",
    "  def position(self, value: np.ndarray):\n",
    "    \"\"\"Set the position of the agent.\"\"\"\n",
    "    self.array[..., State.POSITION_SLICE] = value\n",
    "\n",
    "  @property\n",
    "  def rotation(self) -> np.ndarray:\n",
    "    \"\"\"Return the rotation of the agent. Positive is counter-clockwise.\"\"\"\n",
    "    return self.array[..., State.ROTATION_INDEX]\n",
    "\n",
    "  @rotation.setter\n",
    "  def rotation(self, value: np.ndarray):\n",
    "    \"\"\"Set the rotation of the agent (normalized to [-pi, pi]).\"\"\"\n",
    "    self.array[..., State.ROTATION_INDEX] = normalize_angle(value)\n",
    "\n",
    "  @property\n",
    "  def velocity(self) -> np.ndarray:\n",
    "    \"\"\"Two dimensional velocity vector of the agent.\"\"\"\n",
    "    return self.array[..., State.VELOCITY_SLICE]\n",
    "\n",
    "  @velocity.setter\n",
    "  def velocity(self, value: np.ndarray):\n",
    "    \"\"\"Set the velocity.\"\"\"\n",
    "    self.array[..., State.VELOCITY_SLICE] = value\n",
    "\n",
    "  @property\n",
    "  def angular_velocity(self) -> np.ndarray:\n",
    "    \"\"\"The angular velocity of the agent.\"\"\"\n",
    "    return self.array[..., State.ANGULAR_VELOCITY_INDEX]\n",
    "\n",
    "  @angular_velocity.setter\n",
    "  def angular_velocity(self, value: np.ndarray):\n",
    "    \"\"\"Set the angular velocity of the agent.\"\"\"\n",
    "    self.array[..., State.ANGULAR_VELOCITY_INDEX] = value\n",
    "\n",
    "  @property\n",
    "  def goal1_position(self) -> np.ndarray:\n",
    "    \"\"\"The position of the goal.\"\"\"\n",
    "    return self.array[..., State.GOAL1_POSITION_SLICE]\n",
    "\n",
    "  @goal1_position.setter\n",
    "  def goal1_position(self, value: np.ndarray):\n",
    "    \"\"\"Set the position of the goal.\"\"\"\n",
    "    self.array[..., State.GOAL1_POSITION_SLICE] = value\n",
    "\n",
    "  @property\n",
    "  def has_been_at_goal1(self) -> np.ndarray:\n",
    "    \"\"\"Whether the agent has been at the goal.\"\"\"\n",
    "    return self.array[..., State.HAS_BEEN_AT_GOAL1_INDEX]\n",
    "\n",
    "  @has_been_at_goal1.setter\n",
    "  def has_been_at_goal1(self, value: np.ndarray):\n",
    "    \"\"\"Set whether the agent has been at the goal.\"\"\"\n",
    "    self.array[..., State.HAS_BEEN_AT_GOAL1_INDEX] = value\n",
    "\n",
    "  @property\n",
    "  def goal2_position(self) -> np.ndarray:\n",
    "    \"\"\"The position of the goal.\"\"\"\n",
    "    return self.array[..., State.GOAL2_POSITION_SLICE]\n",
    "\n",
    "  @goal2_position.setter\n",
    "  def goal2_position(self, value: np.ndarray):\n",
    "    \"\"\"Set the position of the goal.\"\"\"\n",
    "    self.array[..., State.GOAL2_POSITION_SLICE] = value\n",
    "\n",
    "  @property\n",
    "  def has_been_at_goal2(self) -> np.ndarray:\n",
    "    \"\"\"Whether the agent has been at the goal.\"\"\"\n",
    "    return self.array[..., State.HAS_BEEN_AT_GOAL2_INDEX]\n",
    "\n",
    "  @has_been_at_goal2.setter\n",
    "  def has_been_at_goal2(self, value: np.ndarray):\n",
    "    \"\"\"Set whether the agent has been at the goal.\"\"\"\n",
    "    self.array[..., State.HAS_BEEN_AT_GOAL2_INDEX] = value\n",
    "\n",
    "  def forward(self, offset: float=0) -> np.ndarray:\n",
    "    \"\"\"The unit vector pointing forward relative to the agent.\"\"\"\n",
    "    return np.stack([np.cos(self.rotation + offset),\n",
    "                     np.sin(self.rotation + offset)], axis=-1)\n",
    "\n",
    "  def right(self, offset: float=0) -> np.ndarray:\n",
    "    \"\"\"The unit vector pointing right relative to the agent.\"\"\"\n",
    "    return self.forward(offset - np.pi / 2)\n",
    "\n",
    "  def at_goal1(self) -> np.ndarray:\n",
    "    \"\"\"Whether the agent is currently at the goal.\"\"\"\n",
    "    return np.linalg.norm(\n",
    "      self.position - self.goal1_position, axis=-1) < State.REACHED_EPSILON\n",
    "\n",
    "  def at_goal2(self) -> np.ndarray:\n",
    "    \"\"\"Whether the agent is currently at the goal.\"\"\"\n",
    "    return np.linalg.norm(\n",
    "      self.position - self.goal2_position, axis=-1) < State.REACHED_EPSILON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop index: 0\n",
      "Stepped game #1\n",
      "Stepped game #1\n",
      "Stepped game #1\n",
      "Stepped game #1\n",
      "Stepped game #1\n",
      "Stepped game #1\n",
      "Stepped game #1\n",
      "Stepped game #1\n",
      "Stepped game #1\n",
      "Stepped game #1\n",
      "Finished Loop #1\n",
      "In Loop #2\n",
      "  Valid observations.\n"
     ]
    }
   ],
   "source": [
    "import abc\n",
    "import asyncio\n",
    "\n",
    "import enact\n",
    "\n",
    "class GameAPI(abc.ABC):\n",
    "  \"\"\"API for the policy to interact with the game.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  async def observations(self):\n",
    "    \"\"\"Used by the policy to read observations.\"\"\"\n",
    " \n",
    "  @abc.abstractmethod\n",
    "  async def step_game(self, actions):\n",
    "    \"\"\"Step the game with the indicated actions.\"\"\"\n",
    "\n",
    "   \n",
    "class PolicyAPI(abc.ABC):\n",
    "  \"\"\"API for the game to interact with the policy.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  async def actions(self):\n",
    "    \"\"\"Used by the game to read actions from the policy.\"\"\"\n",
    " \n",
    "  @abc.abstractmethod\n",
    "  async def step_policy(self, observations):\n",
    "    \"\"\"Step the policy with the indicated observations.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  async def initialize(self, observations):\n",
    "    \"\"\"Initialize the game with a set of observations.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  async def end(self):\n",
    "    \"\"\"Indicates the game has ended.\"\"\"\n",
    " \n",
    "@enact.register\n",
    "class GameEnded(enact.ExceptionResource):\n",
    "  pass\n",
    "\n",
    "class JointAPI(GameAPI, PolicyAPI):\n",
    "  def __init__(self):\n",
    "    self._observations = None\n",
    "    self._actions = None\n",
    "    self._observations_event = asyncio.Event()\n",
    "    self._actions_event = asyncio.Event()\n",
    "  \n",
    "  async def observations(self):\n",
    "    \"\"\"Used by the policy to read observations.\"\"\"\n",
    "    await self._observations_event.wait()\n",
    "    return self._observations\n",
    " \n",
    "  async def actions(self):\n",
    "    \"\"\"Used by the game to read actions.\"\"\"\n",
    "    await self._actions_event.wait()\n",
    "    return self._actions\n",
    " \n",
    "  async def step_policy(self, observations):\n",
    "    await self._actions_event.wait()\n",
    "    self._actions_event.clear()\n",
    "    self._observations = observations\n",
    "    self._observations_event.set()\n",
    " \n",
    "  async def step_game(self, action_arr):\n",
    "    await self._observations_event.wait()\n",
    "    self._observations_event.clear()\n",
    "    self._actions = action_arr\n",
    "    self._actions_event.set()\n",
    " \n",
    "  def initialize(self, observations):\n",
    "    self._observations = observations\n",
    "    self._observations_event.set()\n",
    "\n",
    "  def end(self):\n",
    "    self._observations = None\n",
    "    self._observations_event.set()\n",
    "\n",
    "\n",
    "class Game:\n",
    "\n",
    "  def __init__(self, api: PolicyAPI, observation: State):\n",
    "    self._api = api\n",
    "    self._observation = observation\n",
    "\n",
    "  async def dynamics(self, actions: Action) -> State:\n",
    "    dt = 0.1\n",
    "    next_state = State(np.copy(self._observation.array))\n",
    "    # Update velocity by applying acceleration forces.\n",
    "    torque = actions.torque\n",
    "    thrust = actions.thrust\n",
    "    thrust = np.clip(thrust, 0, 1)\n",
    "    torque = np.clip(torque, -1, 1)\n",
    "    next_state.velocity += (\n",
    "        dt * State.THRUST_FORCE *\n",
    "        self._observation.forward() * np.expand_dims(thrust, axis=-1))\n",
    "    next_state.angular_velocity += dt * State.TORQUE_FORCE * torque\n",
    "\n",
    "    # Clamp max speed and apply friction.\n",
    "    speed = np.clip(\n",
    "        np.expand_dims(np.linalg.norm(next_state.velocity, axis=-1), -1),\n",
    "        1e-10, np.infty)\n",
    "\n",
    "    at_maxed_out_speed = (next_state.velocity / speed) * State.MAX_SPEED\n",
    "    next_state.velocity = np.where(\n",
    "        speed > State.MAX_SPEED, at_maxed_out_speed, next_state.velocity)\n",
    "\n",
    "    next_state.velocity *= State.FRICTION_COEFFICIENT\n",
    "    next_state.angular_velocity *= State.FRICTION_COEFFICIENT\n",
    "\n",
    "    # Update position by applying velocity.\n",
    "    next_state.position += dt * next_state.velocity\n",
    "    next_state.rotation += dt * next_state.angular_velocity\n",
    "\n",
    "    next_state.has_been_at_goal1 += next_state.at_goal1()\n",
    "    next_state.has_been_at_goal1 = np.clip(next_state.has_been_at_goal1, 0, 1)\n",
    "\n",
    "    next_state.has_been_at_goal2 += next_state.at_goal2()\n",
    "    next_state.has_been_at_goal2 = np.clip(next_state.has_been_at_goal2, 0, 1)\n",
    "\n",
    "    return next_state\n",
    "\n",
    "  async def run(self, num_frames: int = 100):\n",
    "    self._api.initialize(self._observation)\n",
    "    for _ in range(num_frames):\n",
    "      actions = await self._api.actions()\n",
    "      # Dynamics update\n",
    "      self._observation = await self.dynamics(actions)\n",
    "      await self._api.step_policy(self._observation)\n",
    "    self._api.end()\n",
    "    \n",
    "\n",
    "@enact.typed_invokable(Action, enact.NoneResource)\n",
    "class TrackedActions(enact.AsyncInvokable):\n",
    "  def __init__(self, api: GameAPI):\n",
    "    self._api = api\n",
    "\n",
    "  async def call(self, actions: Action):\n",
    "    await self._api.step_game(actions)\n",
    "\n",
    "  \n",
    "@enact.typed_invokable(enact.NoneResource, enact.NoneResource)\n",
    "class Policy(enact.AsyncInvokable):\n",
    "\n",
    "  def __init__(self, game_api: GameAPI):\n",
    "    self._api = game_api\n",
    "    self._tracked_actions = TrackedActions(api)\n",
    "  \n",
    "  async def call(self):\n",
    "    idx = 0\n",
    "    while True:\n",
    "      print(f'Loop index: {idx}')\n",
    "      for _ in range(10):\n",
    "        if await self._api.observations() is None:\n",
    "          return  # UGLY\n",
    "        await self._api.step_game(Action(np.array([[1., 1.]])))\n",
    "        print(f'Stepped game #1')\n",
    "      print(f'Finished Loop #1')\n",
    "      for _ in range(10):\n",
    "        print(f'In Loop #2')\n",
    "        if await self._api.observations() is None:\n",
    "          return\n",
    "        print(f'  Valid observations.')\n",
    "        await self._tracked_actions(Action(np.array([[0., 0.]])))\n",
    "        print(f'Stepped game #2')\n",
    "      return\n",
    "\n",
    "\n",
    "api = JointAPI()\n",
    "game = Game(api, State())\n",
    "policy = Policy(api)\n",
    "\n",
    "loop = asyncio.get_running_loop()\n",
    "g = loop.create_task(game.run())\n",
    "with enact.InMemoryStore() as store:\n",
    "  p = loop.create_task(policy.invoke())\n",
    "  _, invocation = await asyncio.gather(g, p)\n",
    "  print(store.get_current())\n",
    "print(store.get_current())\n",
    "with store:\n",
    "  enact.pprint(invocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
